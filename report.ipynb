{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1004eea",
   "metadata": {},
   "source": [
    "# <center>Term project</center>\n",
    "\n",
    "<center>Matti Remes, Lauri Porkka</center>\n",
    "<center>Team name: </center>\n",
    "<center>7.12.2025</center>\n",
    "\n",
    "\n",
    "## <center>Data exploration</center>\n",
    "The first observation made was that the data has 100 features after dropping the columns \"id\", \"date\", \"partlybad\" and \"class4\". \n",
    "\n",
    "\n",
    "## <center>Approach</center>\n",
    "We used a \"grid search\"-method to find the best model and optimize hyperparameters of the model. Grid search does combinations of different models and different hyperparameters of the models and returns the combination that had the best score. In our implementation we first normalized the data. Then we did PCA transformation using different \"cumulative total variance explained\"-hyperparameters. This normalized and transformed data was then fed to the models. Models compared were logistic regression with different penalizations and lambda-parameters, random forest classifier (without PCA) and support vector classifiers. The grid search used accuracy using 5-fold cross-validation to compare the models and different choices of parameters.\n",
    "\n",
    "\n",
    "## <center>Conclusion</center>\n",
    "At the moment logistic regression with lambda value 10 performed the best.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## <center>Grading</center>\n",
    "3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
